<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>My data is bigger than your data!</title>
    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="stylesheets/pygment_trac.css">
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->

<script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+"://platform.twitter.com/widgets.js";fjs.parentNode.insertBefore(js,fjs);}}(document,"script","twitter-wjs");</script>

  </head>
  <body>

<a href="https://github.com/lintool/my-data-is-bigger-than-your-data/"><img style="position: absolute; top: 0; right: 0; border: 0;" src="https://github-camo.global.ssl.fastly.net/38ef81f8aca64bb9a64448d0d70f1308ef5341ab/68747470733a2f2f73332e616d617a6f6e6177732e636f6d2f6769746875622f726962626f6e732f666f726b6d655f72696768745f6461726b626c75655f3132313632312e706e67" alt="Fork me on GitHub" data-canonical-src="https://s3.amazonaws.com/github/ribbons/forkme_right_darkblue_121621.png"></a>

    <div class="wrapper">
      <header>

      </header>
      <section>

<script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>

<h1>My data is bigger than your data!</h1>

<p>How big is big data <i>really</i>? From time to time, various organizations brag about how much data they have, how big their clusters are, how many requests per second they serve, etc. Every time I come across these statistics, I make note of them. It's quite amazing to see how these numbers change over time... looking at the numbers from just a few years ago reminds you of this famous <a href="https://www.youtube.com/watch?v=jTmXHvGZiSY">Austin Powers scene</a>. Here's another <a href="https://www.youtube.com/watch?v=b2F-DItXtZs">gem</a>.</p>

<p>Without further adieu, here's "big" data, in reverse chronological order...</p>

<p><hr/></p>
<h3 style="padding-top: 10px">Cassandra at Apple (June 2019)</h3>

<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">Apple has 160000+ nodes running Cassandra 😯🤯 <a href="https://t.co/hyp41afXpW">https://t.co/hyp41afXpW</a></p>&mdash; Dharmesh Kakadia (@dharmeshkakadia) <a href="https://twitter.com/dharmeshkakadia/status/1132695193910562816?ref_src=twsrc%5Etfw">May 26, 2019</a></blockquote>

<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">Apple’s Cassandra node count in Sep 2014 vs. now ... that’s more than doubled! <a href="https://t.co/v6BIEa4Cn7">pic.twitter.com/v6BIEa4Cn7</a></p>&mdash; Wei Deng (@weideng) <a href="https://twitter.com/weideng/status/1132048243175460864?ref_src=twsrc%5Etfw">May 24, 2019</a></blockquote>


<p><hr/></p>
<h3 style="padding-top: 10px">Internet Archive (May 2018)</h3>

<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">651,621,510,000 web URL&#39;s now in the Wayback Machine by <a href="https://twitter.com/internetarchive?ref_src=twsrc%5Etfw">@internetarchive</a> .   Billions and Billions of web pages!  users hitting &quot;save page now&quot;  at 100 per second:  <a href="https://t.co/AWpIBUrEW1">https://t.co/AWpIBUrEW1</a></p>&mdash; Brewster Kahle (@brewster_kahle) <a href="https://twitter.com/brewster_kahle/status/994380510011928578?ref_src=twsrc%5Etfw">May 10, 2018</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>


<p><hr/></p>
<h3 style="padding-top: 10px">Netflix Statistics on AWS (April 2018)</h3>

<p>A core component of its stream processing system is something called the Keystone data pipeline, which is used to move upwards of 12PB of data per day into its S3 data warehouse, where 100PB of highly compressed data reside.</p>

<p>Keystone Router, a key piece of software that distribute the 3 trillion events per day across 2,000 routing jobs and 200,000 parallel operators to other data sinks in Netflix's S3 repository.</p>

<p>Source: <a href="https://www.datanami.com/2018/04/30/how-netflix-optimized-flink-for-massive-scale-on-aws/">How Netflix Optimized Flink for Massive Scale on AWS</a></p>

<p><hr/></p>
<h3 style="padding-top: 10px">14 TB Hard Drives (April 2018)</h3>

<p><a href="https://www.datanami.com/this-just-in/western-digital-introduces-ultrastar-dc-hc530-14tb-hard-drive/">Western
Digital Introduces Ultrastar DC HC530 14TB Hard Drive.</a></p>

<p><hr/></p>
<h3 style="padding-top: 10px">Plusar and Heron (March 2018)</h3>

<p>Pulsar has been in production for three years at Yahoo, where it
handles 2 million plus topics and processes 100 billion messages per
day.</p>

<p>At Twitter, Heron is used by over 50 teams and processes 2 trillion
events per day, or about 20PB per day.</p>

<p>Source: <a href="https://www.datanami.com/2018/03/06/streamlio-claims-pulsar-performance-advantages-kafka/">Streamlio Claims Pulsar Performance Advantages Over Kafka</a></p>

<p><hr/></p>
<h3 style="padding-top: 10px">Kafka at Netflix: >1 trillion messages per day (March 2018)</h3>

<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">Allen Wang talks about <a href="https://twitter.com/netflix?ref_src=twsrc%5Etfw">@Netflix</a> scalable <a href="https://twitter.com/apachekafka?ref_src=twsrc%5Etfw">@apachekafka</a> architecture at <a href="https://twitter.com/hashtag/qconlondon?src=hash&amp;ref_src=twsrc%5Etfw">#qconlondon</a> with 4000+ brokers and &gt; 1 trillion messages per day. <a href="https://t.co/THpdTWCCRB">pic.twitter.com/THpdTWCCRB</a></p>&mdash; Susanne Kaiser (@suksr) <a href="https://twitter.com/suksr/status/971385672874553344?ref_src=twsrc%5Etfw">March 7, 2018</a></blockquote>


<p><hr/></p>
<h3 style="padding-top: 10px">Prime Day 2017 – Powered by AWS (September 2017)</h3>

<p>During Amazon Prime Day, 2017: Amazon DynamoDB requests from Alexa,
the Amazon.com sites, and the Amazon fulfillment centers totaled 3.34
trillion, peaking at 12.9 million per second.</p>

<p>Source: <a href="https://aws.amazon.com/blogs/aws/prime-day-2017-powered-by-aws/">AWS Blog</a></p>


<p><hr/></p>
<h3 style="padding-top: 10px">Pulsar and Apache DistributedLog (September 2017)</h3>

<p>Apache DistributedLog is a replicated log store originally developed at Twitter. It’s been used in production at Twitter for more than four years, supporting several critical services like pub-sub messaging, log replication for distributed databases, and real-time stream computing, delivering more than 1.5 trillion events (or about 17 PB) per day.</p>

<p>Source: <a href="https://conferences.oreilly.com/strata/strata-ny/public/schedule/detail/60771">Strata Data Conference</a></p>

<p><hr/></p>
<h3 style="padding-top: 10px">European Space Agency's Gaia Mission (August 2017)</h3>

<p>Gaia continues to be a challenging mission in all areas even after
4 years of operation.  In total we have processed almost 800 Billion
(=800,000 Million) astrometric, 160 Billion (=160,000 Million)
photometric and more than 15 Billion spectroscopic observation which
is the largest astronomical dataset from a science space mission until
the present day.</p>

<p>The Gaia mission is considered by the experts “the biggest data
processing challenge to date in astronomy.</p>

<p>Source: <a href="http://www.odbms.org/blog/2017/08/gaia-mission-maps-1-billion-stars-interview-with-uwe-lammers/">ODBMS Blog: Gaia Mission maps 1 Billion stars. Interview with Uwe Lammers</a></p>

<p><hr/></p>
<h3 style="padding-top: 10px">Internet Archive (June 2017)</h3>

<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">Ever wonder how many web URL&#39;s the <a href="https://twitter.com/internetarchive">@internetarchive</a> has archived and is in the wayback?drum roll... 567,683,748,000 (567Billion) as of today</p>&mdash; Brewster Kahle (@brewster_kahle) <a href="https://twitter.com/brewster_kahle/status/877668269846482945">June 21, 2017</a></blockquote>

<p><hr/></p>
<h3 style="padding-top: 10px">Archives Unleashed 4.0 (June 2017)</h3>

<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">Current size o’ the <a href="https://twitter.com/internetarchive">@internetarchive</a>: over 560,000,000,000 URLs collected (plus 2.3m books, 2.4m audio, 3m hours TV, 4m eBooks). <a href="https://twitter.com/hashtag/WAweek2017?src=hash">#WAweek2017</a></p>&mdash; Ian Milligan (@ianmilligan1) <a href="https://twitter.com/ianmilligan1/status/874935070628491264">June 14, 2017</a></blockquote>

<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">.<a href="https://twitter.com/jefferson_bail">@jefferson_bail</a> on Wayback Search: index on anchor text of in-bound links &amp; 443 million homepages, 900 billion link graph. <a href="https://twitter.com/hashtag/WAweek2017?src=hash">#WAweek2017</a></p>&mdash; Ian Milligan (@ianmilligan1) <a href="https://twitter.com/ianmilligan1/status/874935939168174080">June 14, 2017</a></blockquote>

<p>More details at <a href="http://archivesunleashed.com/au4-0-british-invasion/">Archives Unleashed 4.0</a>.</p>

<p><hr/></p>
<h3 style="padding-top: 10px">Spark Summit (June 2017)</h3>

<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">Hello <a href="https://twitter.com/hashtag/SparkSummit2017?src=hash">#SparkSummit2017</a> :) Kicking it off with <a href="https://twitter.com/rxin">@rxin</a>... Here we go! <a href="https://twitter.com/hashtag/sparksummit?src=hash">#sparksummit</a> <a href="https://twitter.com/hashtag/ApacheSpark?src=hash">#ApacheSpark</a> <a href="https://t.co/I4diGSOlZj">pic.twitter.com/I4diGSOlZj</a></p>&mdash; GigaSpaces (@GigaSpaces) <a href="https://twitter.com/GigaSpaces/status/872128429436678144">June 6, 2017</a></blockquote>

<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">Streaming at 65m records/sec with structure steaming <a href="https://twitter.com/hashtag/SparkSummit?src=hash">#SparkSummit</a> <a href="https://t.co/xZ7op98VH9">pic.twitter.com/xZ7op98VH9</a></p>&mdash; Conor (@ConorBMurphy) <a href="https://twitter.com/ConorBMurphy/status/872132055219421184">June 6, 2017</a></blockquote>
<script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>

<p>Addition details can be found on the DataBricks blog: <a href="https://databricks.com/blog/2017/06/06/simple-super-fast-streaming-engine-apache-spark.html">Making Apache Spark the Fastest Open Source Streaming Engine</a></p>

<p><hr/></p>
<h3 style="padding-top: 10px">1 TB graph data in Neo4j (May 2017)</h3>

<p>According to Niels Meersschaert, Chief Technology Officer at
Qualia, the Qualia team relies on over one terabyte of graph data in
Neo4j.</p>

<p>Source: <a href="http://www.odbms.org/blog/2017/05/interview-with-niels-meersschaert/">ODBMS Blog: Identity Graph Analysis at Scale. Interview with Niels Meersschaert</a></p>

<p><hr/></p>
<h3 style="padding-top: 10px">Neo4j Pushes Graph DB Limits Past a Quadrillion Nodes (April 2017)</h3>

<p>A graph database with a quadrillion nodes? Such a monstrous entity
is beyond the scope of what technologist are trying to do now. But
with the latest release of the Neo4j database from Neo Technology,
such a graph is theoretically possible.</p>

<p>Source: <a href="https://www.datanami.com/2016/04/26/neo4j-pushes-graph-db-limits-past-quadrillion-nodes/">datanami</a></p>

<p><hr/></p>
<h3 style="padding-top: 10px">NYSE Data Hub (March 2017)</h3>

<p>The new enterprise data hub supports data processing and analytics
across more than 20 PB of data, with 30 TB of fresh data added
daily.</p>

<p>Source: <a href="https://www.cloudera.com/more/customers/NYSEICE.html">NYSE: Gaining Real-time Insights from More Than 20 PB of Data</a></p>

<p><hr/></p>
<h3 style="padding-top: 10px">Twitter Infrastructure (January 2017)</h3>

<p>Overview of Twitter's current infrastructure:</p>

<img src="images/Twitter2017-infra-distribution.png"/>

<p>Highlights:</p>

<ul>

<li>Hadoop: We have multiple clusters storing over 500 PB divided in four groups (real time, processing, data warehouse and cold storage). Our biggest cluster is over 10k nodes. We run 150k applications and launch 130M containers per day.</li>

<li>Manhattan (the backend for Tweets, Direct Messages, Twitter accounts, and more): We run several clusters for different use cases such as large multi tenant, smaller for non common, read only, and read/write for heavy write/heavy read traffic patterns. The read/only cluster handles 10s of millions QPS whereas a read/write cluster handles millions of QPS. The highest performance cluster, our observability cluster, which ingests in every datacenter, handles over tens of million writes.</li>

<li>Graph: Our legacy Gizzard/MySQL based sharded cluster for storing our graphs. Flock, our social graph, can manage peaks over tens of million QPS, averaging our MySQL servers to 30k - 45k QPS.</li>

<li>Logging (Scribe, but transition over the Flume): We handle over a trillion messages per day and all of these are processed into over 500 categories, consolidated and then and selectively copied across all our clusters.</li>

<li>Caching: We operate hundreds of clusters with an aggregate packet rate of 320M packets/s, delivering over 120GB/s to our clients. We logically partition our caches by users, Tweets, timelines, etc. and in general, every cache cluster is tuned for a particular need. Based on the type of the cluster, they handle between 10M to 50M QPS, and run between hundreds to thousands of instances.</li>

<li>Haplo: Primary cache for Tweet timelines and is backed by a customized version of Redis (implementing the HybridList). Haplo is read-only from Timeline Service and written to by Timeline Service and Fanout Service. Aggregated commands between 40M to 100M per second. Aggregated service requests 800K per second.</li>

</ul>

<p>Source: <a href="https://blog.twitter.com/2017/the-infrastructure-behind-twitter-scale">The Infrastructure Behind Twitter</a>

<p><hr/></p>
<h3 style="padding-top: 10px">Internet Archive (January 2017)</h3>

<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">532,461,462,000 URL&#39;s in the new Wayback index! go <a href="https://twitter.com/internetarchive">@internetarchive</a> (this includes all the images and such, so the page count is ~ 279B)</p>&mdash; Brewster Kahle (@brewster_kahle) <a href="https://twitter.com/brewster_kahle/status/816772509546598401">January 4, 2017</a></blockquote>
<script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>

<p><hr/></p>
<h3 style="padding-top: 10px">Kafka at Uber (December 2016)</h3>

<p>Since January 2016 Chaperone has been a key piece of Uber
Engineering’s multi–data center infrastructure, currently handling
about a trillion messages a day.</p>

<p>Source: <a href="https://eng.uber.com/chaperone/">Introducing Chaperone: How Uber Engineering Audits Kafka End-to-End</a></p>


<p><hr/></p>
<h3 style="padding-top: 10px">Internet Archive (October 2016)</h3>

<p>In October of 2012, we held just over 10 petabytes of unique content. Today, we have archived a little over 30 petabytes, and we add between 13 and 15 terabytes of content per day (web and television are the most voluminous).</p>

<p>Currently, Internet Archive hosts about 20,000 individual disk drives. Each of these are housed in specialized computers (we call them “datanodes”) that have 36 data drives (plus two operating systems drives) per machine. Datanodes are organized into racks of 10 machines (360 data drives), and interconnected via high-speed ethernet to form our storage cluster. Even though our content storage has tripled over the past four years, our count of disk drives has stayed about the same. This is because disk drive technology improvements. Datanodes that were once populated with 36 individual 2-terabyte (2T) drives are today filled with 8-terabyte (8T) drives, moving single node capacity from 72 terabytes (64.8T formatted) to 288 terabytes (259.2T formatted) in the same physical space! This evolution of disk density did not happen in a single step, so we have populations of 2T, 3T, 4T, and 8T drives in our storage clusters.</p>

<p>Source: <a href="http://blog.archive.org/2016/10/25/20000-hard-drives-on-a-mission/">Internet Archive Blog</a></p>

<p><hr/></p>
<h3 style="padding-top: 10px">Metamarkets (September 2016)</h3>

<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">Catch Xavier Léauté discussing streaming analytics at 300B events per day with Kafka, Samza and Druid <a href="https://t.co/Mq95Ueufdt">https://t.co/Mq95Ueufdt</a> <a href="https://twitter.com/hashtag/StrataHadoop?src=hash">#StrataHadoop</a></p>&mdash; Confluent (@ConfluentInc) <a href="https://twitter.com/ConfluentInc/status/781554755810320384">September 29, 2016</a></blockquote>

<p>Today Metamarkets processes over 300 billion events per day, representing over 100 TB going through a single pipeline built entirely on open source technologies including Druid, Kafka, and Samza. Growing to such a scale presents engineering challenges on many levels, not just in design but also with operations, especially when downtime is not an option.</p>

<p>Source: <a href="http://conferences.oreilly.com/strata/hadoop-big-data-ny/public/schedule/detail/51152">Strata+Hadoop World</a></p>

<p><hr/></p>
<h3 style="padding-top: 10px">Facebook (August 2016)</h3>

<p>A single Spark job that reads 60 TB of compressed data and performs a 90 TB shuffle and sort.</p>

<p>Source: <a href="https://code.facebook.com/posts/1671373793181703">Facebook Engineering</a></p>

<p><hr/></p>
<h3 style="padding-top: 10px">Samsung's New SSD (August 2016)</h3>

<p>Samsung is showing off a monster million IOPS SSD that can pump out
read data at 6.4 gigabytes per second and store up to 6.4TB.</p>

<p>Source: <a href="http://www.theregister.co.uk/2016/08/30/samsung_pm1725a_ssd_release_vmworld/">The Register</a>

<p><hr/></p>
<h3 style="padding-top: 10px">Tesla (May 2016)</h3>

<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">Tesla logs more sensor miles daily than Google has in its history <a href="https://twitter.com/hashtag/EmTechDigital?src=hash">#EmTechDigital</a> <a href="https://twitter.com/techreview">@techreview</a> <a href="https://t.co/cnf4bspU4t">https://t.co/cnf4bspU4t</a> <a href="https://t.co/XNoAWaH2LY">pic.twitter.com/XNoAWaH2LY</a></p>&mdash; Steve Jurvetson (@dfjsteve) <a href="https://twitter.com/dfjsteve/status/735189237218189312">May 24, 2016</a></blockquote>

<p>At ‪#‎EmTechDigital: Tesla now gains a million miles of driving data
(comparing human to robot safety) every 10 hours. So they log more
miles per day than the Google program has gathered since
inception. Tesla's design goal is to be 2-10x better than human
drivers.</p>

<p>Source: <a href="https://www.facebook.com/jurvetson/posts/10156961026885611">Steve Jurvetson</a></p>

<p><hr/></p>
<h3 style="padding-top: 10px">Google Translate (May 2016)</h3>

<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">143,280,496,726: The number of words Google translates per day into over 100 languages.</p>&mdash; Delip Rao (@deliprao) <a href="https://twitter.com/deliprao/status/733852055643660288">May 21, 2016</a></blockquote>

<p>Google translates more than 100 billion words each day.</p>

<p>Source: <a href="https://googleblog.blogspot.com/2016/04/ten-years-of-google-translate.html">Official Google Blog</a></p>

<p>Related, Google CEO Sundar Pichai announced during his Google I/O
keynote that 20 percent of queries on its mobile app and on Android
devices are voice searches.</p>

<p>Source: <a href="http://searchengineland.com/google-reveals-20-percent-queries-voice-queries-249917">Search Engine Land</a></p>

<p><hr/></p>
<h3 style="padding-top: 10px">Google App Engine (April 2016)</h3>

<p>Google App Engine serves over 100 billion requests per day.</p>

<p>Source: <a href="https://cloudplatform.googleblog.com/2016/04/lessons-from-a-Google-App-Engine-SRE-on-how-to-serve-over-100-billion-requests-per-day.html">Google blog</a></p>

<p><hr/></p>
<h3 style="padding-top: 10px">What happens in a 2016 internet minute? (April 2016)</h3>

<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">Much of the Internet r the &quot;dark matters&quot; of the cyber universe - pictures &amp; videos. Our work is to illuminate them! <a href="https://t.co/SkGNcA4Fxl">pic.twitter.com/SkGNcA4Fxl</a></p>&mdash; Fei-Fei Li (@drfeifei) <a href="https://twitter.com/drfeifei/status/722018340605001729">April 18, 2016</a></blockquote>

<p><hr/></p>
<h3 style="padding-top: 10px">Scale of Spotify (March 2016)</h3>

<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr"><a href="https://twitter.com/hashtag/Spotify?src=hash">#Spotify</a> numbers: 75M users, 30M songs, 1.5B playlists, 1TB user data every day<a href="https://twitter.com/hashtag/ecir2016?src=hash">#ecir2016</a> <a href="https://twitter.com/ecir2016">@ecir2016</a> <a href="https://twitter.com/hashtag/industryday?src=hash">#industryday</a></p>&mdash; Gianmaria Silvello (@giansilv) <a href="https://twitter.com/giansilv/status/712582456579387392">March 23, 2016</a></blockquote>

<p><hr/></p>
<h3 style="padding-top: 10px">EMC Unveils DSSD D5: A Quantum Leap In Flash Storage (February 2016)</h3>

<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">Congrats <a href="https://twitter.com/EMCDSSD">@EMCDSSD</a> <a href="https://twitter.com/jmc_cool">@jmc_cool</a> <a href="https://twitter.com/cj_desai">@cj_desai</a> <a href="https://twitter.com/jburton">@jburton</a> <a href="https://twitter.com/hashtag/FaceMelting?src=hash">#FaceMelting</a> <a href="https://twitter.com/hashtag/RackScale?src=hash">#RackScale</a> Flash <a href="https://twitter.com/hashtag/DSSD?src=hash">#DSSD</a> D5 <a href="https://t.co/CFUoBKD9Q6">https://t.co/CFUoBKD9Q6</a> <a href="https://t.co/sczFoZBN1O">pic.twitter.com/sczFoZBN1O</a></p>&mdash; Michael Dell (@MichaelDell) <a href="https://twitter.com/MichaelDell/status/708033559345233920">March 10, 2016</a></blockquote>

<ul>
<li>EMC DSSD D5 delivers superior IOPS, throughput and latency metrics for the most data-intensive applications to enable complex, real-time data processing and real-time analytics and insight</li>
<li>10M IOPS, ~100 microsecond latency, 100GB/s of bandwidth and 144TBs of raw storage in five rack units</li>
<li>Up to 68% lower TCO, 5X lower latency and 10X higher IOPS and bandwidth than today's fastest flash platforms</li>
<li>Order of magnitude improvement in Hadoop HBase workload performance compared to traditional Hadoop deployed on Direct Attached Storage (DAS)</li>
</ul>

<p>Source: <a href="http://www.emc.com/about/news/press/2016/20160229-03.htm">EMC Press Release</a></p>

<p><hr/></p>
<h3 style="padding-top: 10px">1PB Uploaded to YouTube Every Day (February 2016)</h3>

<p>For YouTube alone, users upload over 400 hours of video every minute, which at one gigabyte per hour requires more than one petabyte (1M GB) of new storage every day or about 100x the Library of Congress. As shown in the graph, this continues to grow exponentially, with a 10x increase every five years.</p>

<img src="images/2016-02-23-youtube.png"/>

<p>Source: <a href="http://googlecloudplatform.blogspot.ca/2016/02/Google-seeks-new-disks-for-data-centers.html">Google Cloud Platform Blog</a></p>

<p><hr/></p>
<h3 style="padding-top: 10px">Sequencing vs Moore's Law (January 2016)</h3>

<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">For those that haven&#39;t seen the updated Sequencing vs Moore&#39;s Law chart, we did pretty good in 2015 <a href="https://t.co/pYTCxsXBRj">pic.twitter.com/pYTCxsXBRj</a></p>&mdash; Marc Chevrette (@wildtypeMC) <a href="https://twitter.com/wildtypeMC/status/689654478224556032">January 20, 2016</a></blockquote>

<p>Source: <a href="http://www.genome.gov/sequencingcosts/">DNA Sequencing Costs: Data from the NHGRI Genome Sequencing Program (GSP)</a></p>

<p><hr/></p>
<h3 style="padding-top: 10px">One Trillion Messages per Day with Kafka (November 2015)</h3>

<blockquote class="twitter-tweet" lang="en"><p lang="en" dir="ltr">. <a href="https://twitter.com/apachekafka">@apachekafka</a> powers <a href="https://twitter.com/Microsoft">@Microsoft</a> Bing, Ads and Office. 1 trillion messages/day, 1000+ brokers, 5 million/sec peak <a href="https://t.co/pQm63BUA5I">pic.twitter.com/pQm63BUA5I</a></p>&mdash; Neha Narkhede (@nehanarkhede) <a href="https://twitter.com/nehanarkhede/status/667903877769891840">November 21, 2015</a></blockquote>

<p><hr/></p>
<h3 style="padding-top: 10px">ONE MILLION HTTP requests per second (November 2015)</h3>

<p>Demo of a Google Container Engine cluster serving 1 million HTTP requests per second.</p>

<p>Source: <a href="http://blog.kubernetes.io/2015/11/one-million-requests-per-second-dependable-and-dynamic-distributed-systems-at-scale.html">Kubernetes blog</a></p>

<p><hr/></p>
<h3 style="padding-top: 10px">Spark at Scale (November 2015)</h3>

<p><img src="images/IEEE-bigdata-2015-keynote.png" width="600"/>

<p>Source: Keynote by Ion Stocia at <a href="http://cci.drexel.edu/bigdata/bigdata2015/Keynotes.html">IEE Big Data 2015</a></p>

<p><hr/></p>
<h3 style="padding-top: 10px">Big data... blast from the past</h3>

<img src="tweets/655306196128280576.png">

<!--blockquote class="twitter-tweet" lang="en"><p lang="en" dir="ltr">email from 1996 <a href="http://t.co/e4l4GxiSJt">pic.twitter.com/e4l4GxiSJt</a></p>&mdash; Alice Maz (@alicemazzy) <a href="https://twitter.com/alicemazzy/status/655306196128280576">October 17, 2015</a></blockquote-->

<p><hr/></p>
<h3 style="padding-top: 10px">Large Scale Distributed Deep Learning on Hadoop Clusters (September 2015)</h3>

<p>Yahoo has a current footprint of more than 40,000 servers and 600
petabytes of storage spread across 19 clusters.</p>

<p>Source: <a href="http://yahoohadoop.tumblr.com/post/129872361846/large-scale-distributed-deep-learning-on-hadoop">Yahoo! blog</a></p>

<p><hr/></p>
<h3 style="padding-top: 10px">Big Code at Google (September 2015)</h3>

<p>Google has around two billion lines of code totaling around 85 TB,
all in a mono-repo. There are around 45,000 commits per day from
25,000 engineers. See
talk <a href="https://www.youtube.com/watch?v=W71BTkUbdqE">here</a>.</p>

<p><img src="images/2015-09-23-Google-source1.png"/></p>

<p><img src="images/2015-09-23-Google-source2.png"/></p>

<p>Source: <a href="http://www.wired.com/2015/09/google-2-billion-lines-codeand-one-place/">Wired</a></p>

<p><hr/></p>
<h3 style="padding-top: 10px">Hadoop at Twitter (September 2015)</h3>

<p>Our Hadoop filesystems host over 300PB of data on tens of thousands of servers. Blog post below further describes Twitter's federated, multi-DC Hadoop setup.</p>

<p>Source: <a href="https://blog.twitter.com/2015/hadoop-filesystem-at-twitter">Hadoop filesystem at Twitter</a></p>

<p><hr/></p>
<h3 style="padding-top: 10px">HBase at Bloomberg (September 2015)</h3>

<p>
The Bloomberg end of day historical system is called PriceHistory...
This system was designed for the high volume of requests for single securities and quite impressive for its day.
The response time for a year's data for a field on a security is 5ms, and it receives billions of hits a day, around half a million a second at peak.</p>

<p>Source: <a href="https://blogs.apache.org/hbase/entry/medium_data_and_universal_data">Apache Blog</a></p>

<p><hr/></p>
<h3 style="padding-top: 10px">Kafka at LinkedIn handles 1.1 Trillion Messages Per Day (September 2015)</h3>

<blockquote class="twitter-tweet" lang="en"><p lang="da" dir="ltr">Kafka at LinkedIn handles 1.1 Trillion messages written per day <a href="http://t.co/BPdHSSub5S">http://t.co/BPdHSSub5S</a></p>&mdash; Jay Kreps (@jaykreps) <a href="https://twitter.com/jaykreps/status/639138439477067777">September 2, 2015</a></blockquote>

<p>Source: <a href="http://www.confluent.io/blog/apache-kafka-hits-1.1-trillion-messages-per-day-joins-the-4-comma-club">Confluent blog post</a></p>

<p><hr/></p>
<h3 style="padding-top: 10px">Scalable of the Go language (July 2015)</h3>

<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">Ever hear that <a href="https://twitter.com/hashtag/golang?src=hash">#golang</a> doesn&#39;t scale? Explain Baidu&#39;s HTTP/S service - 100 billion requests/day. 100% <a href="https://twitter.com/hashtag/golang?src=hash">#golang</a>. <a href="http://t.co/zyKR98CnbH">pic.twitter.com/zyKR98CnbH</a></p>&mdash; Jason Buberel (@jbuberel) <a href="https://twitter.com/jbuberel/status/617776229437980673">July 5, 2015</a></blockquote>
<script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>

<p><hr/></p>
<h3 style="padding-top: 10px">Hadoop Summit (June 2015)</h3>

<blockquote class="twitter-tweet" lang="en"><p lang="en" dir="ltr">See you at the <a href="https://twitter.com/hashtag/hadoopsummit?src=hash">#hadoopsummit</a> Thursday! Hear <a href="https://twitter.com/timt">@timt</a> talk abt Flurry Analytics and the 5+ PB we have in HBase <a href="http://t.co/nQSMMfNJrU">http://t.co/nQSMMfNJrU</a></p>&mdash; Flurry (@FlurryMobile) <a href="https://twitter.com/FlurryMobile/status/608351677427679232">June 9, 2015</a></blockquote>

<blockquote class="twitter-tweet" lang="en"><p lang="en" dir="ltr"><a href="https://twitter.com/hashtag/hadoopsummit?src=hash">#hadoopsummit</a> 2015 flurry uses <a href="https://twitter.com/hashtag/hbase?src=hash">#hbase</a> heavily with 5+ PB <a href="http://t.co/H3R23eoQwF">pic.twitter.com/H3R23eoQwF</a></p>&mdash; Subash D&#39;Souza (@sawjd22) <a href="https://twitter.com/sawjd22/status/609028501392347137">June 11, 2015</a></blockquote>

<blockquote class="twitter-tweet" lang="en"><p lang="en" dir="ltr"><a href="https://twitter.com/sawjd22">@sawjd22</a> <a href="https://twitter.com/Tshooter">@Tshooter</a> <a href="https://twitter.com/twitter">@twitter</a> its 300+ PB :-)</p>&mdash; Sudhir Rao (@ysudhir) <a href="https://twitter.com/ysudhir/status/608390407513358336">June 9, 2015</a></blockquote>

<p><hr/></p>
<h3 style="padding-top: 10px">Kafka at Uber (May 2015)</h3>

<blockquote class="twitter-tweet" lang="en"><p lang="en" dir="ltr">Uber engineer: <a href="https://twitter.com/apachekafka">@apachekafka</a> forms the backbone of Uber&#39;s real-time dispatch infrastructure. Processes 10s of billions of events/day</p>&mdash; Neha Narkhede (@nehanarkhede) <a href="https://twitter.com/nehanarkhede/status/600846134039318528">May 20, 2015</a></blockquote>

<p><hr/></p>
<h3 style="padding-top: 10px">Yahoo's Pistachio (May 2015)</h3>

<p>Pistachio is Yahoo's recently open-sourced distributed key value store system:<p>

<blockquote>
It’s being used as the user profile storage for large scale ads serving products within Yahoo. 10+ billions of user profiles are being stored with ~2 million reads QPS, 0.8GB/s read throughput and ~0.5 million writes QPS, 0.3GB/s write throughput. Average latency is under 1ms. It guarantees strong consistency and fault-tolerance. We have hundreds of servers in 8 data centers all over the globe supporting hundreds of millions in revenue.
</blockquote>

<p/>

<p>Source: <a href="http://yahooeng.tumblr.com/post/116291838351/pistachio-co-locate-the-data-and-compute-for">Yahoo! Engineering Blog</a></p>

<p><hr/></p>
<h3 style="padding-top: 10px">Updates form HBaseCon (May 2015)</h3>

<blockquote class="twitter-tweet" lang="en"><p lang="en" dir="ltr"><a href="https://twitter.com/hashtag/HBaseCon?src=hash">#HBaseCon</a> <a href="https://twitter.com/hashtag/google?src=hash">#google</a> <a href="https://twitter.com/hashtag/bigtable?src=hash">#bigtable</a> statistics <a href="http://t.co/o4tULk8Q8J">pic.twitter.com/o4tULk8Q8J</a></p>&mdash; Subash D&#39;Souza (@sawjd22) <a href="https://twitter.com/sawjd22/status/596353886262136832">May 7, 2015</a></blockquote>

<blockquote class="twitter-tweet" lang="en"><p lang="en" dir="ltr">Watch <a href="https://twitter.com/jeremy_carroll">@jeremy_carroll</a>&#39;s talk on HBase at scale <a href="https://twitter.com/Pinterest">@Pinterest</a>. An online, high-demand environment with ~5M operations per second! <a href="https://twitter.com/hashtag/HBaseCon?src=hash">#HBaseCon</a></p>&mdash; karan gupta (@karan) <a href="https://twitter.com/karan/status/596344098119819264">May 7, 2015</a></blockquote>

<p><hr/></p>
<h3 style="padding-top: 10px">Bay Area Mesos Meetup (April 2015)</h3>

<blockquote class="twitter-tweet" lang="en"><p lang="en" dir="ltr">nice to be at the Bay Area Mesos meet up tonight and see Apple announce that Siri is powered by <a href="https://twitter.com/ApacheMesos">@ApacheMesos</a> <a href="http://t.co/kEphLs7Rfk">pic.twitter.com/kEphLs7Rfk</a></p>&mdash; Chris Aniszczyk (@cra) <a href="https://twitter.com/cra/status/591063478049222656">April 23, 2015</a></blockquote>

<p><a href="https://mesosphere.com/blog/2015/04/23/apple-details-j-a-r-v-i-s-the-mesos-framework-that-runs-siri/">Blog post</a> describing how Siri's backend is powered by Apache Mesos.</p>

<p><hr/></p>
<h3 style="padding-top: 10px">Statistics from the Hadoop Summit in Brussels (April 2015)</h3>

<blockquote class="twitter-tweet" lang="en"><p>Yahoo stats at <a href="https://twitter.com/hashtag/hadoopsummit?src=hash">#hadoopsummit</a> - 1 million jobs per day! <a href="http://t.co/UAsV4yPEND">pic.twitter.com/UAsV4yPEND</a></p>&mdash; Andy Leaver (@AndyLeaver) <a href="https://twitter.com/AndyLeaver/status/588624402683547648">April 16, 2015</a></blockquote>

<blockquote class="twitter-tweet" lang="en"><p>Yahoo numbers again: 600 petabytes, 43 000 servers, 1 million jobs/day <a href="https://twitter.com/hashtag/hadoopsummit?src=hash">#hadoopsummit</a> <a href="http://t.co/9EMGndZiqb">pic.twitter.com/9EMGndZiqb</a></p>&mdash; Bence Arato (@BenceArato) <a href="https://twitter.com/BenceArato/status/588623433266765824">April 16, 2015</a></blockquote>

<blockquote class="twitter-tweet" lang="en"><p>“People do stupid stuff all the time; <a href="https://twitter.com/hashtag/Hadoop?src=hash">#Hadoop</a> lets them do stupid stuff at scale...” Yahoo Admin (via <a href="https://twitter.com/owen_omalley">@owen_omalley</a>) <a href="https://twitter.com/hashtag/HadoopSummit?src=hash">#HadoopSummit</a></p>&mdash; Jos van Dongen (@josvandongen) <a href="https://twitter.com/josvandongen/status/588022832178823170">April 14, 2015</a></blockquote>

<blockquote class="twitter-tweet" lang="en"><p>Spotify’s net Hadoop data volume is 13 PB (plus replication), but to my surprise it doesn’t include any music (stored on S3). <a href="https://twitter.com/hashtag/HadoopSummit?src=hash">#HadoopSummit</a></p>&mdash; Andy Bitterer (@bitterer) <a href="https://twitter.com/bitterer/status/588009191647895555">April 14, 2015</a></blockquote>

<blockquote class="twitter-tweet" lang="en"><p>3B ads displayed daily, 6 <a href="https://twitter.com/hashtag/datacenter?src=hash">#datacenter</a> on 3 continents. Find out how <a href="https://twitter.com/criteo">@criteo</a> manages <a href="https://twitter.com/hashtag/BigData?src=hash">#BigData</a> at our booth at <a href="https://twitter.com/hashtag/hadoopsummit?src=hash">#hadoopsummit</a></p>&mdash; Criteo Labs (@CriteoEng) <a href="https://twitter.com/CriteoEng/status/588640705154781184">April 16, 2015</a></blockquote>

<p><hr/></p>
<h3 style="padding-top: 10px">Hadoop at Twitter (March 2015)</h3>

<blockquote class="twitter-tweet" lang="en"><p>We launch ~200M containers per day on our Hadoop clusters <a href="https://twitter.com/twitterhadoop">@twitterhadoop</a>. Yeah that’s &gt; 2.3K/s on average. <a href="https://twitter.com/hashtag/scale?src=hash">#scale</a> <a href="https://twitter.com/hashtag/Hadoop?src=hash">#Hadoop</a></p>&mdash; Joep R. (@joep) <a href="https://twitter.com/joep/status/573233399655100416">March 4, 2015</a></blockquote>


<p><hr/></p>
<h3 style="padding-top: 10px">The Internet? Bah! (February 2015)</h3>

<blockquote class="twitter-tweet" lang="en"><p>20 years ago this week, Newsweek published the wrongest column ever. <a href="http://t.co/ap8vtRnpIZ">http://t.co/ap8vtRnpIZ</a> <a href="https://twitter.com/hashtag/NewsweekRewind?src=hash">#NewsweekRewind</a> <a href="http://t.co/8LHAKpw4rE">pic.twitter.com/8LHAKpw4rE</a></p>&mdash; Newsweek (@Newsweek) <a href="https://twitter.com/Newsweek/status/571768557358358528">February 28, 2015</a></blockquote>


<p><hr/></p>
<h3 style="padding-top: 10px">The Explosive Growth of Spark (February 2015)</h3>

<blockquote class="twitter-tweet" lang="en"><p>Interest in Spark is overwhelming on Stack Overflow. <a href="https://twitter.com/hashtag/StrataHadoop?src=hash">#StrataHadoop</a> <a href="http://t.co/amReOTetLk">pic.twitter.com/amReOTetLk</a></p>&mdash; Donnie Berkholz (@dberkholz) <a href="https://twitter.com/dberkholz/status/568561792751771648">February 20, 2015</a></blockquote>


<p><hr/></p>
<h3 style="padding-top: 10px">Kafka (February 2015)</h3>

<p>Today at LinkedIn Kafka handles over 500 billion events per day
spread over a number of data centers. It became the backbone for data
flow between systems of all kinds, the core pipeline for Hadoop data,
and the hub for stream processing.</p>

<p>Source: <a href="http://blog.confluent.io/2015/02/25/stream-data-platform-1/">Confluence Blog</a></p>
<p><hr/></p>
<h3 style="padding-top: 10px">Strata (February 2015)</h3>

<blockquote class="twitter-tweet" lang="en"><p><a href="https://twitter.com/hashtag/LinkedIn?src=hash">#LinkedIn</a> @ <a href="https://twitter.com/hashtag/StrataConf?src=hash">#StrataConf</a>: 80000 QPS, 24 server clusters, up to 100 nodes/cluster, 99.99% latency on <a href="https://twitter.com/hashtag/Couchbase?src=hash">#Couchbase</a> <a href="http://t.co/iK4CpGGpmg">pic.twitter.com/iK4CpGGpmg</a> <a href="https://twitter.com/hashtag/nosql?src=hash">#nosql</a></p>&mdash; Vincent Gonnot (@vgonnot) <a href="https://twitter.com/vgonnot/status/568703775936028672">February 20, 2015</a></blockquote>


<p><hr/></p>
<h3 style="padding-top: 10px">Andrew Moore shares his experiences at Google (January 2015)</h3>

<p><a href="http://www.cs.cmu.edu/~awm/">Andrew Moore</a> (CMU professor who built Google Pittsburgh and helped grow Google's Adwords and shopping systems) shared his experiences at the <a href="http://workshops.cs.georgetown.edu/BDSI-2015/">NITRD Big Data Strategic Initiative Workshop</a> held at Georgetown in January, 2015. I attended the workshop and captured a few tidbits:</p>

<blockquote class="twitter-tweet" lang="en"><p>Andrew Moore: Google&#39;s ecommerce platform ingests 100K-200K events per second continuously. <a href="http://t.co/55eM351eJG">http://t.co/55eM351eJG</a></p>&mdash; Jimmy Lin (@lintool) <a href="https://twitter.com/lintool/status/558637581605535744">January 23, 2015</a></blockquote>


<blockquote class="twitter-tweet" lang="en"><p>Andrew Moore: Google&#39;s ecommerce machine-learned models based on data from last 9 months. Interesting. <a href="http://t.co/55eM351eJG">http://t.co/55eM351eJG</a></p>&mdash; Jimmy Lin (@lintool) <a href="https://twitter.com/lintool/status/558641992935440384">January 23, 2015</a></blockquote>


<blockquote class="twitter-tweet" lang="en"><p>Andrew Moore proposes a &quot;data science&quot; stack: device, &quot;kernel&quot; (e.g., kv stores), machine learning, modeling, decision support.</p>&mdash; Jimmy Lin (@lintool) <a href="https://twitter.com/lintool/status/558645184398102530">January 23, 2015</a></blockquote>


<blockquote class="twitter-tweet" lang="en"><p>Andrew Moore admits, &quot;computer scientists at Google have gotten fat and lazy&quot; with all the available compute resources.</p>&mdash; Jimmy Lin (@lintool) <a href="https://twitter.com/lintool/status/558646614139879424">January 23, 2015</a></blockquote>


<blockquote class="twitter-tweet" lang="en"><p>Andrew Moore: there are some solutions outside Google that are just as good, but require 3 orders magnitude less hardware resources.</p>&mdash; Jimmy Lin (@lintool) <a href="https://twitter.com/lintool/status/558646939173269506">January 23, 2015</a></blockquote>


<p><hr/></p>

<h3 style="padding-top: 10px">Petabyte Sort with Spark (November 2014)</h3>

<p>Spark sorts 100 TB in 23 minutes on 206 machines (6592 virtual
cores), which translates into 4.27 TB/min or 20.7 GB/min/node.</p>

<p>Spark sorts 1 PB in 234 minutes on 190 machines (6080 virtual cores),
which translates into 4.27 TB/min or 22.5 GB/min/node.</p>

<p>Source: <a href="http://databricks.com/blog/2014/11/05/spark-officially-sets-a-new-record-in-large-scale-sorting.html">Databricks blog</a> and also <a href="https://databricks.com/blog/2014/10/10/spark-petabyte-sort.html">this older post</a>.</p>

<p><hr/></p>

<h3 style="padding-top: 10px">Hadoop at Yahoo (September 2014)</h3>

<p>From an interview with Mithun Radhakrishnan, member of the Yahoo Hive team:</p>

<p><blockquote>"Y!Grid is Yahoo's Grid of Hadoop Clusters that's used for all the "big data" processing that happens in Yahoo today. It currently consists of 16 clusters in multiple datacenters, spanning 32,500 nodes, and accounts for almost a million Hadoop jobs every day. Around 10% of those are Hive jobs."</blockquote></p>

<p>Source: <a href="http://www.odbms.org/blog/2014/09/interview-mithun-radhakrishnan/">ODBMS Industry Watch</a></p>

<p><hr/></p>
<h3 style="padding-top: 10px">Gartner: Hype Cycle for Big Data (August 2014)</h3>

<p>Big data has just passed the top of the Hype Cycle, and moving toward the Trough of Disillusionment. This is not a bad thing. It means the market starts to mature, becoming more realistic about how big data can be useful for organizations &nbsp; large and small. Big data will become business as usual.</p>

<p>Source: <a href="https://www.gartner.com/doc/2814517/hype-cycle-big-data-">Gartner</a></p>

<p><hr/></p>
<h3 style="padding-top: 10px">Twitter (July 2014)</h3>

<blockquote class="twitter-tweet" lang="en"><p>Always exciting when your Hadoop 2 clusters grow beyond 5000 machines <a href="https://twitter.com/twitterhadoop">@twitterhadoop</a> cc <a href="https://twitter.com/corestorage">@corestorage</a></p>&mdash; Joep R. (@joep) <a href="https://twitter.com/joep/statuses/494236903395168256">July 29, 2014</a></blockquote>


<p><hr/></p>
<h3 style="padding-top: 10px">Pinterest (July 2014)</h3>

<p>Some stats:</p>

<ul>

  <li>30 billion Pins in the system</li>
  <li>20 terabytes of new data each day</li>
  <li>around 10 petabytes of data in S3</li>
  <li>migrated Hadoop jobs to Qubole</li>
  <li>over 100 regular MapReduce users running over 2,000 jobs each day through Qubole's web interface (ad-hoc jobs and scheduled workflows)</li>
  <li>six standing Hadoop clusters comprised of over 3,000 nodes</li>
  <li>over 20 billion log messages and process nearly a petabyte of data with Hadoop each day</li>

</ul>

<p>Source: <a href="http://engineering.pinterest.com/post/92742371919/powering-big-data-at-pinterest">Pinterest Engineering Blog</a></p>

<p><hr/></p>
<h3 style="padding-top: 10px">Baidu (July 2014)</h3>

<blockquote class="twitter-tweet" lang="en"><p><a href="https://twitter.com/hashtag/sigir2014?src=hash">#sigir2014</a> <a href="https://twitter.com/hashtag/sirip?src=hash">#sirip</a> Baidu processes 7 billion searches per day.</p>&mdash; Mark Sanderson (@IR_oldie) <a href="https://twitter.com/IR_oldie/statuses/485937075951112193">July 7, 2014</a></blockquote>


<p><hr/></p>
<h3 style="padding-top: 10px">HDFS at Twitter (June 2014)</h3>

<blockquote class="twitter-tweet" lang="en"><p>Max int in javascript 2^53=8 PB isn&#39;t enough to show some individual users&#39; HDFS usage on our <a href="https://twitter.com/hashtag/Hadoop?src=hash">#Hadoop</a> clusters. Overflow is so 20th century!</p>&mdash; Joep R. (@joep) <a href="https://twitter.com/joep/statuses/482343633761222656">June 27, 2014</a></blockquote>


<p><hr/></p>
<h3 style="padding-top: 10px">ACL Lifetime Achievement Award (June 2014)</h3>

<blockquote class="twitter-tweet" lang="en"><p>A perspective on progress of computing from Bob Mercer&#39;s ACL lifetime award talk. <a href="https://twitter.com/hashtag/acl2014?src=hash">#acl2014</a> <a href="http://t.co/DQ0j5NF8wI">pic.twitter.com/DQ0j5NF8wI</a></p>&mdash; Delip Rao (@deliprao) <a href="https://twitter.com/deliprao/statuses/481925555177025536">June 25, 2014</a></blockquote>


<p><hr/></p>
<h3 style="padding-top: 10px">Airbnb (June 2014)</h3>

<blockquote class="twitter-tweet" lang="en"><p>15 million total guests have stayed on Airbnb. It took us nearly 4 years to get our 1st million, and now we have 1M guests every month.</p>&mdash; Brian Chesky (@bchesky) <a href="https://twitter.com/bchesky/statuses/476515613880614913">June 11, 2014</a></blockquote>


<p><hr/></p>
<h3 style="padding-top: 10px">DataTorrent at the Hadoop Summit (June 2014)</h3>

<p>[David] Hornik [of August Capital] was also an early stage investor in Splunk, and he sees lots of potential here for DataTorrent. “When you can process a billion data points in a second, there are a lot of possibilities.”</p>

<p>Source: <a href="http://techcrunch.com/2014/06/03/datatorrent-big-data-processing-platform-built-on-hadoop-can-process-a-billion-events-per-second/">TechCrunch</a></p>

<p>What would you do with a system that could process 1.5 billion events per second? That’s the mind-boggling rate at which DataTorrent’s Real-Time Streaming (RTS) offering for Hadoop was recently benchmarked. Now that RTS is generally available–DataTorrent announced its general availability today at Hadoop Summit in San Jose–we may soon find out.<p>

<p>That 1.5-billion-events-per-second figure was recorded on DataTorrent’s internal Hadoop cluster, which sports 34 nodes. Each node is able to process tens of thousands of incoming events (call data records, machine data, and clickstream data are common targets) per second, and in turn generates hundreds of thousands of secondary events that are then processed again using one of the 400 operators that DataTorrent makes available as part of its in-memory, big-data kit.</p>

<p>Source: <a href="http://www.datanami.com/2014/06/03/datatorrent-rts-clocks-1-5b-events-per-second/">Datanami</a></p>

<p><hr/></p>
<h3 style="padding-top: 10px">Yahoo! and the Hadoop Summit (June 2014)</h3>

<blockquote class="twitter-tweet" lang="en"><p><a href="https://twitter.com/search?q=%23hadoopsummit&amp;src=hash">#hadoopsummit</a> <a href="https://twitter.com/Yahoo">@yahoo</a> - benchmark for interactive analytics: 60B events, 3.5TB data compressed, response time of &lt;400ms. WOW! <a href="https://twitter.com/QlikView">@QlikView</a></p>&mdash; bob hardaway (@bobhardaway) <a href="https://twitter.com/bobhardaway/statuses/474254415335919616">June 4, 2014</a></blockquote>


<blockquote class="twitter-tweet" lang="en"><p>Wondering how big Hadoop clusters get? Yahoo is approaching 500 PB <a href="https://twitter.com/search?q=%23hadoopsummit&amp;src=hash">#hadoopsummit</a> <a href="https://twitter.com/search?q=%23bigdatap&amp;src=hash">#bigdatap</a> <a href="http://t.co/osNZrZR5ir">http://t.co/osNZrZR5ir</a></p>&mdash; richardwinter (@richardwinter) <a href="https://twitter.com/richardwinter/statuses/473911597434667009">June 3, 2014</a></blockquote>

<p>[<a href="images/2014-06-03-Yahoo.jpg">local cached copy</a>]</p>


<p><hr/></p>
<h3 style="padding-top: 10px">Snapchat (May 2014)</h3>

<p>Snapchat claims that over 700 million snaps are shared per day on the service, which could make it the most-used photo-sharing app in the world — ahead of Facebook, WhatsApp, and others. Even Snapchat’s Stories feature seems to be doing well, amassing 500 million views per day.</p>

<p>Source: <a href="http://www.theverge.com/2014/5/1/5670260/real-talk-the-new-snapchat-makes-texting-fun-again-video-calls">The Verge</a></p>

<p><hr/></p>
<h3 style="padding-top: 10px">Kafka at LinkedIn (May 2014)</h3>

<p><img src="images/2014-05-27-Kafka.png" width="450px"/></p>

<p>Source: <a href="https://speakerdeck.com/ept/samza-at-linkedin-taking-stream-processing-to-the-next-level">Samza at LinkedIn: Taking Stream Processing to the Next Level</a> by Martin Kleppmann at Berlin Buzzwords on May 27, 2014</p>

<p><hr/></p>
<h3 style="padding-top: 10px">What "Big Data" Means to Google (May 2014)</h3>

<blockquote class="twitter-tweet" lang="en"><p>David Glazer showing what <a href="https://twitter.com/search?q=%23bigdata&amp;src=hash">#bigdata</a> means <a href="https://twitter.com/google">@google</a> <a href="https://twitter.com/search?q=%23bigdatamed&amp;src=hash">#bigdatamed</a> <a href="http://t.co/8YlDhfqJOW">pic.twitter.com/8YlDhfqJOW</a></p>&mdash; Andrew Su (@andrewsu) <a href="https://twitter.com/andrewsu/statuses/469575062547218432">May 22, 2014</a></blockquote>


<p><hr/></p>
<h3 style="padding-top: 10px">Tape Storage (May 2014)</h3>

<blockquote class="twitter-tweet" data-cards="hidden" lang="en"><p>Tale of the tape: 400+ Exabytes of data are stored on tape! <a href="https://t.co/3SX9AGWq1i">https://t.co/3SX9AGWq1i</a> <a href="https://twitter.com/search?q=%23tapeworldrecord&amp;src=hash">#tapeworldrecord</a> <a href="http://t.co/G0r1GuUFP0">http://t.co/G0r1GuUFP0</a></p>&mdash; IBM Research (@IBMResearch) <a href="https://twitter.com/IBMResearch/statuses/468721591338422274">May 20, 2014</a></blockquote>


<p>Here's a <a href="images/ibm-tape-2014.jpg">local copy of infographic</a>.</p>

<p>Here's <a href="http://www-03.ibm.com/press/us/en/pressrelease/43945.wss">more bragging</a> from IBM about achieving a new record of 85.9 billion bits of data per square inch in areal data density on low-cost linear magnetic particulate tape.</p>

<p><hr/></p>
<h3 style="padding-top: 10px">Google's Bigdata at HBaseCon2014 (May 2014)</h3>

<p>Bigtable scale numbers from keynote talk at <a href="http://hbasecon.com/agenda/">HBaseCon2014</a> by Carter Page:

<p><blockquote class="twitter-tweet" lang="en"><p>Correction: BigTable at Google serves 2+ exabytes at 600M QPS organization wide. That&#39;s a scale quite challenging to conceptualize. Wow.</p>&mdash; Andrew Purtell (@akpurtell) <a href="https://twitter.com/akpurtell/statuses/463747917782589441">May 6, 2014</a></blockquote>
</p>

<p>More <a href="http://seen.co/event/hbasecon-2014-san-francisco-ca-2014-9905/highlight/35873">HBaseCon2014 highlights</a>.</p>

<p><hr/></p>
<h3 style="padding-top: 10px">Hadoop at eBay (April 2014)</h3>

<p><blockquote class="twitter-tweet" lang="en"><p>EBay&#39;s scaling of their Hadoop clusters is impressive. <a href="http://t.co/SVNf42LAMI">pic.twitter.com/SVNf42LAMI</a></p>&mdash; Owen O&#39;Malley (@owen_omalley) <a href="https://twitter.com/owen_omalley/statuses/451686067490410497">April 3, 2014</a></blockquote>
</p>

<p><hr/></p>
<h3 style="padding-top: 10px">Hive at Facebook (April 2014)</h3>

<p>Our warehouse stores upwards of 300 PB of Hive data, with an incoming daily rate of about 600 TB.</p>

<p>Source: <a href="https://code.facebook.com/posts/229861827208629/scaling-the-facebook-data-warehouse-to-300-pb/">Facebook Engineering Blog</a></p>

<p><hr/></p>
<h3 style="padding-top: 10px">Flurry's HBase Cluster (April 2014)</h3>

<p>Flurry has the largest contiguous HBase cluster: Mobile analytics company Flurry has an HBase cluster with 1,200 nodes (replicating into another 1,200 node cluster).</p>

<p><i>Editorial Note:</i> Really? Even larger than Facebook's HBase cluster that powers Messages?</p>

<p>Source: <a href="http://radar.oreilly.com/2014/04/5-fun-facts-about-hbase-that-you-didnt-know.html">O'Reilly Radar</a></p>

<p><hr/></p>
<h3 style="padding-top: 10px">Kafka at LinkedIn (April 2014)</h3>

<p><blockquote class="twitter-tweet" lang="en"><p>Kafka metrics @ LinkedIN 300 Brokers, 18000 topics, 220 Billions messages per day... impressive! <a href="https://twitter.com/search?q=%23apachecon&amp;src=hash">#apachecon</a></p>&mdash; Ronan GUILLAMET (@_Spiff_) <a href="https://twitter.com/_Spiff_/statuses/453297030073307136">April 7, 2014</a></blockquote>
</p>

<p><hr/></p>
<h3 style="padding-top: 10px">Internet Archive (February 2014)</h3>

<p><blockquote class="twitter-tweet" lang="en"><p>Wayback Machine updated, now 397,144,266,000 web objects in it, (html, jpg, css). Getting close to 400Billion. <a href="https://twitter.com/internetarchive">@internetarchive</a></p>&mdash; Brewster Kahle (@brewster_kahle) <a href="https://twitter.com/brewster_kahle/statuses/439414396108800000">February 28, 2014</a></blockquote>
</p>

<p><hr/></p>
<h3 style="padding-top: 10px">How Google Backs Up The Internet (January 2014)</h3>

<p>Talk <a href="https://www.youtube.com/watch?v=eNliOm9NtCM">How Google Backs Up the Internet</a>. Here's a <a href="http://highscalability.com/blog/2014/2/3/how-google-backs-up-the-internet-along-with-exabytes-of-othe.html">nice blog post</a> summarizing talk contents.</p>

<p>Interesting tidbits:</p>

<ul>

  <li>GMail alone is approaching low exabytes of data.</li>

  <li>A lot of the backup processes are coordinated by MapReduce.</li>

  <li>Google deploys what amounts to be RAID4 on tape.</li>

  <li>The GMail outage of 2011 is what compelled Google to reveal that it uses tapes for backup. Interesting that this talk was largely focused on tape, perhaps because of the GMail outage case study?</li>

  <li>From a question: deduplication isn't always a win; sometimes faster to just keep multiple copies.</li>

</ul>

<p>Other (personal) observations:</p>

<ul>

<li>One of the questioners
mentioned <a href="https://github.com/Netflix/SimianArmy">Chaos
Monkey</a> and asked if Google did something like that. The speaker
didn't seem to know what Chaos Monkey was, which is a bit surprising
since that tool has gotten a fair bit a publicity in the
community. This is completely anecdotal and speculative of course,
but perhaps this speaks to the insular nature of Google's culture?</li>

<li>One of the questioners didn't know what MapReduce was and asked
about it. The speaker said "I don't actually know MapReduce all that
well... I just use it." Then he started talking about semaphores and
locking. While nothing he said was technically incorrect, it was a
fairly inarticulate answer for a fairly straightforward
question. Weird.</li>

</ul>

<p><hr/></p>
<h3 style="padding-top: 10px">Google's BigTable (September 2013)</h3>

<blockquote class="twitter-tweet" lang="en"><p>Jeff Dean at XLDB: Largest Google Bigtable cluster: ~100s PB data; sustained: 30M ops/sec; 100+ GB/s I/O (gmail?) <a href="https://t.co/BbL8J2VNB4">https://t.co/BbL8J2VNB4</a></p>&mdash; Jimmy Lin (@lintool) <a href="https://twitter.com/lintool/statuses/380015731691106304">September 17, 2013</a></blockquote>


<p>Source: <a href="https://conf-slac.stanford.edu/xldb-2013/sites/conf-slac.stanford.edu.xldb-2013/files/JDean.pdf">Jeff Dean's talk slides at XLDB 2013</a> [<a href="data/2013-09-10-JeffDean.pdf">local copy</a>]</p>

<p><hr/></p>
<h3 style="padding-top: 10px">Google's Disks (2013)</h3>

<p>Estimate: Google has close to 10 exabytes of active storage
attached to running clusters.</p>

<p>Source: <a href="http://what-if.xkcd.com/63/">What if?</a></p>

<p>For reference: Total disk storage systems capacity shipped (in 2013) reached
8.2 exabytes.</p>

<p>Source: <a href="https://web.archive.org/web/20130911091903/http://www.idc.com/getdoc.jsp?containerId=prUS24302513">IDC Press Release</a></p>

<p><hr/></p>
<h3 style="padding-top: 10px">NSA's datacenter (Summer 2013)</h3>

<ul>
<li><a href="http://www.forbes.com/sites/kashmirhill/2013/07/24/blueprints-of-nsa-data-center-in-utah-suggest-its-storage-capacity-is-less-impressive-than-thought/">Blueprints Of NSA's Ridiculously Expensive Data Center In Utah Suggest It Holds Less Info Than Thought</a></li>
<li><a href="http://www.wired.com/threatlevel/2012/03/ff_nsadatacenter/all/1">The NSA Is Building the Country's Biggest Spy Center (Watch What You Say)</a></li>
<li><a href="http://luxexumbra.blogspot.ca/2013/08/capacity-of-utah-data-center.html"</a>Capacity of the Utah Data Center</a></li>
</ul>

<p><hr/></p>
<h3 style="padding-top: 10px">Facebook's Giraph (August 2013)</h3>

<p>All of these improvements have made Giraph fast, memory efficient, scalable and easily integrated into our existing Hive data warehouse architecture. On 200 commodity machines we are able to run an iteration of page rank on an actual 1 trillion edge social graph formed by various user interactions in under four minutes with the appropriate garbage collection and performance tuning. We can cluster a Facebook monthly active user data set of 1 billion input vectors with 100 features into 10,000 centroids with k-means in less than 10 minutes per iteration.</p>

<p>Source: <a href="https://code.facebook.com/posts/509727595776839/scaling-apache-giraph-to-a-trillion-edges/">Facebook Engineering</a></p>

<p><hr/></p>
<h3 style="padding-top: 10px">Hadoop Summit (June 2013)</h3>

<blockquote class="twitter-tweet" lang="en"><p>oh thoseYyahoo stats, like 365 PB storage, 330k YARN nodes, ~4- 6 YARN jobs per second <a href="https://twitter.com/search?q=%23HadoopSummit&amp;src=hash">#HadoopSummit</a> <a href="https://twitter.com/search?q=%23bigdata&amp;src=hash">#bigdata</a> term sounds almost quaint</p>&mdash; Tony Baer (@TonyBaer) <a href="https://twitter.com/TonyBaer/statuses/349943270119452672">June 26, 2013</a></blockquote>


<blockquote class="twitter-tweet" lang="en"><p><a href="https://twitter.com/search?q=%23Hadoop&amp;src=hash">#Hadoop</a> at Yahoo - 365+ PB of HDFS storage, 30,000 nodes, 400,000 jobs per day, 10 million hours per day <a href="https://twitter.com/search?q=%23hadoopsummit&amp;src=hash">#hadoopsummit</a></p>&mdash; Jeff Kelly (@jeffreyfkelly) <a href="https://twitter.com/jeffreyfkelly/statuses/349943221117403136">June 26, 2013</a></blockquote>

<p><hr/></p>
<h3 style="padding-top: 10px">Amazon S3 (April 2013)</h3>

<p>There are now more than 2 trillion objects stored in Amazon S3 and that the service is regularly peaking at over 1.1 million requests per second.</p>

<p>Source: <a href="http://aws.typepad.com/aws/2013/04/amazon-s3-two-trillion-objects-11-million-requests-second.html">Amazon Web Services Blog</a></p>

<p><hr/></p>
<h3 style="padding-top: 10px">Hadoop Summit (March 2013)</h3>

<blockquote class="twitter-tweet" lang="en"><p lang="en" dir="ltr">Neustar went from storing 1% of data for 60 days at $100k/TB on EDW to 100% of data for over a year at $500/TB on Hadoop <a href="https://twitter.com/hashtag/hadoopsummit?src=hash">#hadoopsummit</a></p>&mdash; Matt Aslett (@maslett) <a href="https://twitter.com/maslett/status/314661192763326465">March 21, 2013</a></blockquote>

<p><hr/></p>
<h3 style="padding-top: 10px">Strata (February 2013)</h3>

<blockquote class="twitter-tweet" lang="en"><p>Some EA video games produce over 1 TB of data per day. Analyze if users are playing the game the way it is intended. <a href="https://twitter.com/hashtag/strataconf?src=hash">#strataconf</a></p>&mdash; Aaron Lewing (@_aaroneous) <a href="https://twitter.com/_aaroneous/status/306811751469551616">February 27, 2013</a></blockquote>


<blockquote class="twitter-tweet" lang="en"><p>275 million users generate 50 TB of data a day for <a href="https://twitter.com/EA">@ea</a> <a href="https://twitter.com/hashtag/strataconf?src=hash">#strataconf</a> (cc <a href="https://twitter.com/kon_s">@kon_s</a> ) <a href="http://t.co/dgHXDDGADF">http://t.co/dgHXDDGADF</a></p>&mdash; Joerg Blumtritt (@jbenno) <a href="https://twitter.com/jbenno/status/306815085052047360">February 27, 2013</a></blockquote>


<p><hr/></p>
<h3 style="padding-top: 10px">Hadoop at Yahoo! (February 2013)</h3>

<p>Around ~45k hadoop nodes, ~350 PB total</p>

<img src="images/hadoopatyahoo.png" width="600px"/>

<p>Source: <a href="http://developer.yahoo.com/blogs/ydn/posts/2013/02/hadoop-at-yahoo-more-than-ever-before/">YDN Blog</a></p>

<p><hr/></p>
<h3 style="padding-top: 10px">Internet Archive reaches 10 PB (October 2012)</h3>

<p><a href="http://blog.archive.org/2012/10/10/the-ten-petabyte-party/">Blog post</a> about the Internet Archive's 10 PB party.</p>

<p><blockquote class="twitter-tweet" lang="en"><p>10,000,000,000,000,000 Bytes Archived! <a href="https://twitter.com/internetarchive">@internetarchive</a> go open content movement <a href="http://t.co/UDO1Vpwd">http://t.co/UDO1Vpwd</a></p>&mdash; Brewster Kahle (@brewster_kahle) <a href="https://twitter.com/brewster_kahle/statuses/261864516202156032">October 26, 2012</a></blockquote>
</p>

<p><blockquote class="twitter-tweet" lang="en"><p><a href="https://twitter.com/search?q=%23WaybackMachine&amp;src=hash">#WaybackMachine</a> updated with 240 billion pages! Go <a href="https://twitter.com/internetarchive">@internetarchive</a> ! 5PB be <a href="https://twitter.com/search?q=%23BigData&amp;src=hash">#BigData</a> <a href="http://t.co/vrAq9NL3">http://t.co/vrAq9NL3</a></p>&mdash; Brewster Kahle (@brewster_kahle) <a href="https://twitter.com/brewster_kahle/statuses/289418815094284288">January 10, 2013</a></blockquote>
</p>

<p>Source: <a href="http://blog.archive.org/2013/01/09/updated-wayback/">Internet Archive</a></p>

<p><hr/></p>
<h3 style="padding-top: 10px">Google at SES San Francisco (August 2012)</h3>

<p>Google has seen more than 30 trillion URLs and crawls 20 billion pages
a day. One hundred billion searches are conducted each month on Google
(3 billion a day).</p>

<p>Source: <a href="http://searchenginewatch.com/article/2199092/Spotlight-Keynote-With-Matt-Cutts-SESSF">Spotlight Keynote With Matt Cutts #SESSF (from Google)</a></p>

<p><hr/></p>
<h3 style="padding-top: 10px">Cassandra at eBay (August 2012)</h3>

<p style="margin-bottom: 0in;">eBay Marketplaces:</p>
<ul>
<li>97 million active buyers and sellers </li>
<li>200+ million items </li>
<li>2 billion page views each day </li>
<li>80 billion database calls each day </li>
<li>5+ petabytes of site storage capacity </li>
<li>80+ petabytes of analytics storage capacity</li>
</ul>

<p style="margin-bottom: 0in;">A glimpse on our Cassandra deployment:</p>
<ul>
<li>Dozens of nodes across multiple clusters </li>
<li>200 TB+ storage provisioned </li>
<li>400M+ writes & 100M+ reads per day, and growing </li>
<li>QA, LnP, and multiple Production clusters</li>
</ul>

<p>Source: <a href="http://www.slideshare.net/jaykumarpatel/cassandra-at-ebay-13920376">Slideshare</a> [<a href="data/cassandraatebay-120809022318-phpapp01.pdf">local copy</a>]</p>

<p><hr/></p>
<h3 style="padding-top: 10px">The size, scale, and numbers of eBay.com (June 2012)</h3>

<ul>
<li>We have over 10 petabytes of data stored in our <a href="http://hadoop.apache.org">Hadoop</a> and <a href="http://teradata.com">Teradata</a> clusters. Hadoop is primarily used by engineers who use data to build products, and Teradata is primarily used by our finance team to understand our business</li>
<li>We have over 300 million items for sale, and over a billion accessible at any time (including, for example, items that are no longer for sale but that are used by customers for price research)</li>
<li>We process around 250 million user queries per day (which become many billions of queries behind the scenes – <a href="http://hughewilliams.com/2012/03/19/query-rewriting-in-search-engines/">query rewriting</a> implies many calls to search to provide results for a single user query, and many other parts of our system use search for various reasons)</li>
<li>We serve over 2 billion pages to customers every day</li>
<li>We have over 100 million active users</li>
<li>We sold over US$68 billion in merchandize in 2011</li>
<li>We make over 75 billion database calls each day (our database tables are denormalized because doing relational joins at our scale is often too slow – and so we precompute and store the results, leading to many more queries that take much less time each)</li>
</ul>

<p>Source: <a href="http://hughewilliams.com/2012/06/26/the-size-scale-and-numbers-of-ebay-com/">Hugh Williams Blog Post</a></p>

<p><hr/></p>
<h3 style="padding-top: 10px">Facebook at Hadoop Summit (June 2012)</h3>

<blockquote class="twitter-tweet" lang="en"><p>HDFS growth at <a href="https://twitter.com/search?q=%23facebook&amp;src=hash">#facebook</a>. 100 PB in some clusters. 200 million files <a href="https://twitter.com/search?q=%23hadoopsummit&amp;src=hash">#hadoopsummit</a> <a href="http://t.co/qyaMaper">pic.twitter.com/qyaMaper</a></p>&mdash; chiradeep (@chiradeep) <a href="https://twitter.com/chiradeep/statuses/212972081921536000">June 13, 2012</a></blockquote>


<p><hr/></p>
<h3 style="padding-top: 10px">Square Kilometer Array (June 2012)</h3>

<blockquote class="twitter-tweet" lang="en"><p>Listening to NASA big data challenges at <a href="https://twitter.com/hashtag/hadoopSummit?src=hash">#hadoopSummit</a>, the square kilometer array project will produce 700tb per second. TB. Per second.</p>&mdash; Matt Winkler (@mwinkle) <a href="https://twitter.com/mwinkle/status/213024236703453185">June 13, 2012</a></blockquote>


<blockquote class="twitter-tweet" lang="en"><p>Holy crap. Square kilometer array will generate 700 TB/sec. ... What? <a href="https://twitter.com/hashtag/NASA?src=hash">#NASA</a> <a href="https://twitter.com/hashtag/HadoopSummit?src=hash">#HadoopSummit</a></p>&mdash; Jonathan Natkins (@nattyice) <a href="https://twitter.com/nattyice/status/213019905317027840">June 13, 2012</a></blockquote>


<p><hr/></p>
<h3 style="padding-top: 10px">Amazon S3 Cloud Storage Hosts 1 Trillion Objects (June 2012)</h3>

<p>Late last week the number of objects stored in Amazon S3 reached one trillion.</p>

<p>Source: <a href="http://aws.typepad.com/aws/2012/06/amazon-s3-the-first-trillion-objects.html">Amazon Web Services Blog</a></p>

<p><hr/></p>
<h3 style="padding-top: 10px">Pinterest Architecture Update (May 2012)</h3>

<p>18 Million Visitors, 10x Growth, 12 Employees, 410 TB Of Data.</p>

<p>80 million objects stored in S3 with 410 terabytes of user data, 10x what they had in August. EC2 instances have grown by 3x.  Around $39K fo S3 and $30K for EC2.</p>

<p>Source: <a href="http://highscalability.com/blog/2012/5/21/pinterest-architecture-update-18-million-visitors-10x-growth.html">High Scalability Blog Post</a></p>

<p><hr/></p>
<h3 style="padding-top: 10px">Six Super-Scale Hadoop Deployments (April 2012)</h3>

<p>Source: <a href="http://www.datanami.com/datanami/2012-04-26/six_super-scale_hadoop_deployments.html?page=1">Datanami</a></a></p>

<p><hr/></p>
<h3 style="padding-top: 10px">Ranking at eBay (April 2012)</h3>

<p>eBay is amazingly dynamic. Around 10% of the 300+ million items for sale end each day (sell or end unsold), and a new 10% is listed. A large fraction of items have updates: they get bids, prices change, sellers revise descriptions, buyers watch, buyers offer, buyers ask questions, and so on. We process tens of millions of change events on items in a typical day, that is, our search engine receives that many signals that something important has changed about an item that should be used in the search ranking process. And all that is happening while we process around 250 million queries on a typical day.</p>

<p>Source: <a href="http://hughewilliams.com/2012/04/19/ranking-at-ebay-part-1/">Hugh Williams Blog Post</a></p>

<p><hr/></p>
<h3 style="padding-top: 10px">Modern HTTP Servers Are Fast (March 2012)</h3>

<p>A modern HTTP server (nginx 1.0.14) running on somewhat recent hardware (dual Intel Xeon X5670, 6 cores at 2.93 GHz, with 24GB of RAM) is capable of servicing 500,000 Requests/Sec.</p>

<p>Source: <a href="http://lowlatencyweb.wordpress.com/2012/03/20/500000-requestssec-modern-http-servers-are-fast/">The Low Latency Web</a></p>

<p><hr/></p>
<h3 style="padding-top: 10px">Strata (February 2012)</h3>

<blockquote class="twitter-tweet" lang="en"><p>A 777 will generate 1 TB of data from all the sensors during a 3 hr flight! Crazy. <a href="https://twitter.com/hashtag/strataconf?src=hash">#strataconf</a> <a href="https://twitter.com/hashtag/jumpstart?src=hash">#jumpstart</a></p>&mdash; Rob May (@robmay) <a href="https://twitter.com/robmay/status/174557786720579584">February 28, 2012</a></blockquote>


<p><hr/></p>
<h3 style="padding-top: 10px">Tumblr Architecture (February 2012)</h3>

<p>15 Billion Page Views A Month And Harder To Scale Than Twitter:  500 million page views a day, a peak rate of ~40k requests per second, ~3TB of new data to store a day, all running on 1000+ servers.</p>

<p>Source: <a href="http://highscalability.com/blog/2012/2/13/tumblr-architecture-15-billion-page-views-a-month-and-harder.html">High Scalability Blog Post</a></p>

<p><hr/></p>
<h3 style="padding-top: 10px">Digital Universe (2011)</h3>

<p>In 2011 the world will create a staggering 1.8 zettabytes.</p>

<p>Source: <a href="http://www.emc.com/leadership/programs/digital-universe.htm">IDC</a></p>

<p><hr/></p>
<h3 style="padding-top: 10px">DataSift Architecture (November 2011)</h3>

<ul>
<li>936 CPU Cores</li>
<li>Current Peak Delivery of 120,000 Tweets Per Second (260Mbit bandwidth) </i>
<li>Performs 250+ million sentiment analysis with sub 100ms latency </li>
<li>1TB of augmented (includes gender, sentiment, etc) data transits the platform daily</li>
<li>Data Filtering Nodes Can process up to 10,000 unique streams (with peaks of 8000+ tweets running through them per second) </li>
<li>Can do data-lookup's on 10,000,000+ username lists in real-time </li>
<li>Links Augmentation Performs 27 million link resolves + lookups plus 15+ million full web page aggregations per day.</li>
</ul>

<p>Source: <a href="http://highscalability.com/blog/2011/11/29/datasift-architecture-realtime-datamining-at-120000-tweets-p.html">High Scalability Blog Post</a></p>

<p><hr/></p>
<h3 style="padding-top: 10px">Hadoop at Facebook (July 2011)</h3>

<p>In 2010, Facebook had the largest Hadoop cluster in the world, with over 20 PB of storage. By March 2011, the cluster had grown to 30 PB.</p>

<p>Source: <a href="http://www.facebook.com/notes/paul-yang/moving-an-elephant-large-scale-hadoop-data-migration-at-facebook/10150246275318920">Facebook Engineering Blog</a></p>

<p><hr/></p>
<h3 style="padding-top: 10px">Facebook (June 2010)</h3>

<blockquote class="twitter-tweet" lang="en"><p>Facebook Hadoop: 36 PB of uncompressed data, &gt;2250 machines, 23,000 cores, 32 GB of RAM per machine, processing 80-90TB/day. <a href="https://twitter.com/hashtag/hadoopsummit?src=hash">#hadoopsummit</a></p>&mdash; Ron Bodkin (@ronbodkin) <a href="https://twitter.com/ronbodkin/status/17354887236">June 29, 2010</a></blockquote>


<p><hr/></p>
<h3 style="padding-top: 10px">Hive at Facebook (May 2009)</h3>

<ul>
<li>Facebook has 400 terabytes of disk 	managed by Hadoop/Hive, with a slightly better than 6:1 overall 	compression ratio. So the 2 1/2 petabytes figure for user 	data is reasonable.</li>
<li>Facebook&#8217;s Hadoop/Hive system 	ingests 15 terabytes of new data per day now, not 10.</li>
<li>Hadoop/Hive cycle times aren&#8217;t as 	fast as I thought I heard from Jeff.  Ad targeting queries are the 	most frequent, and they&#8217;re run hourly. Dashboards are 	repopulated daily.</li>
</ul>
<p>In a new-to-me metric, Facebook has 610 Hadoop nodes, running in a single cluster, due to be increased to 1000 soon.</p>

<p>Source: <a href="http://www.dbms2.com/2009/05/11/facebook-hadoop-and-hive/">DBMS2</a></p>

<p><hr/></p>
<h3 style="padding-top: 10px">Datawarehouses at eBay (April 2009)</h3>

<p style="margin-bottom: 0in;">Metrics on eBay's main Teradata data warehouse include:</p>
<ul>
<li>&gt;2 petabytes of user data</li>
<li>10s of 1000s of users</li>
<li>Millions of queries per day</li>
<li>72 nodes</li>
<li>&gt;140 GB/sec of I/O, or 2	GB/node/sec, or maybe that's a peak when the workload is scan-heavy</li>
<li>100s of production databases being 	fed in</li>
</ul>
<p style="margin-bottom: 0in;">Metrics on eBay's Greenplum data warehouse (or, if you like, data mart) include:</p>
<ul>
<li>6 1/2 petabytes of user data</li>
<li>17 trillion records</li>
<li>150 billion new records/day, <span>which seems to suggest an ingest rate well over </span>50 terabytes/day</li>
<li>96 nodes</li>
<li>200 MB/node/sec of I/O (that's the order of magnitude difference that triggered my post on 	disk drives)</li>
<li>4.5 petabytes of storage</li>
<li>70% compression</li>
<li>A small number of concurrent users</li>
</ul>

<p>Source: <a href="http://www.dbms2.com/2009/04/30/ebays-two-enormous-data-warehouses/">DBMS2</a></p>

<p><hr/></p>
<h3 style="padding-top: 10px">The World's Technological Capacity (2007)</h3>

<p>In 2007, humankind was able to store 295 exabytes.</p>

<p>Source: <a href="http://www.sciencemag.org/content/332/6025/60.full/">Science Magazine</a></p>

<p>All the empty or usable space on hard drives, tapes, CDs, DVDs, and memory (volatile and nonvolatile) in the market equaled 264 exabytes.</p>

<p>Source: <a href="http://www.emc.com/collateral/analyst-reports/diverse-exploding-digital-universe.pdf">IDC</a></p>

<p><hr/></p>
<h3 style="padding-top: 10px">Visa Credit Card Transactions (2007)</h3>

<p>According to the Visa website, they processed 27.612 billion
transactions in 2007. This means an average of 875 credit transactions
per second based on a uniform distribution. Assuming that 80% of
transactions occur in the 8 hours of the day, this gives an event rate
of 2100 transactions per second.</p>

<p>Source: <a href="http://www.doc.ic.ac.uk/~migliava/papers/09-debs-next.pdf">Schultz-Møller et al. (2009)</a>

</section>
      <footer>

        <p><small>Theme based on <a href="https://github.com/orderedlist">orderedlist</a></small></p>
      </footer>
    </div>
    <script src="js/scale.fix.js"></script>
  </body>
</html>
